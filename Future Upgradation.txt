The submitted assignment demonstrated potential for future improvement, hindered by limited resources. While GPT-Neo was employed for text-to-text generation, yielding somewhat enhanced captions compared to the generated image descriptions, the process required the input of multiple examples, resulting in increased computation time. Addressing this challenge entails leveraging alternative language models such as GPT-3 or QLLora, renowned for their heightened capabilities attributed to a larger number of parameters. These advanced models have the potential to perform such tasks more effectively, obviating the necessity for multiple examples and thereby reducing computation time. However, it is crucial to consider the associated computational requirements, including cost and infrastructure implications. Moreover, exploring optimization techniques and model compression methods can help mitigate the computational burden of working with larger language models without compromising performance. Consequently, with technological advancements and the availability of more potent models, promising opportunities for future enhancements in the assignment's objectives of efficient and effective text-to-text generation emerge.